
## Substitutos para LogisticRegression (Modelos de ClassificaÃ§Ã£o)

### 1. **Decision Tree Classifier**

```python
from sklearn.tree import DecisionTreeClassifier
modelo = DecisionTreeClassifier()
```

* ğŸŒ³ Baseado em regras tipo "if... else..."
* âœ… InterpretaÃ§Ã£o fÃ¡cil
* âš ï¸ Pode sofrer com overfitting


### 2. **Random Forest Classifier**

```python
from sklearn.ensemble import RandomForestClassifier
modelo = RandomForestClassifier()
```

* ğŸŒ² Conjunto de vÃ¡rias Ã¡rvores de decisÃ£o
* âœ… Mais robusto que Decision Tree
* ğŸ”„ Usa mÃ©dia das previsÃµes das Ã¡rvores
* ğŸ“ˆ Boa performance geral


### 3. **K-Nearest Neighbors (KNN)**

```python
from sklearn.neighbors import KNeighborsClassifier
modelo = KNeighborsClassifier(n_neighbors=5)
```

* ğŸ“ Classifica com base nos vizinhos mais prÃ³ximos
* âœ… Simples e eficaz para dados pequenos
* âš ï¸ Lento para grandes volumes de dados


### 4. **Support Vector Machine (SVM)**

```python
from sklearn.svm import SVC
modelo = SVC()
```

* ğŸ“Š Tenta encontrar o melhor "limite" entre classes
* âœ… Eficiente em espaÃ§os de alta dimensÃ£o
* âš ï¸ Pode ser lento e difÃ­cil de ajustar


### 5. **Naive Bayes**

```python
from sklearn.naive_bayes import GaussianNB
modelo = GaussianNB()
```

* ğŸ§® Baseado em probabilidade (Teorema de Bayes)
* âœ… RÃ¡pido e eficiente
* âš ï¸ SupÃµe independÃªncia entre variÃ¡veis (nem sempre Ã© o caso)


### 6. **Gradient Boosting Classifier**

```python
from sklearn.ensemble import GradientBoostingClassifier
modelo = GradientBoostingClassifier()
```

* ğŸ“ˆ MÃ©todo de boosting (modelo aprende com os erros anteriores)
* âœ… Alta acurÃ¡cia
* âš ï¸ Mais lento para treinar, mas muito eficaz


### 7. **XGBoost / LightGBM / CatBoost** (Modelos externos)

* Requerem instalaÃ§Ã£o extra:

```bash
pip install xgboost lightgbm catboost
```

* Exemplo com XGBoost:

```python
from xgboost import XGBClassifier
modelo = XGBClassifier()
```

* ğŸš€ Muito usados em competiÃ§Ãµes de machine learning (Kaggle)
* âœ… AltÃ­ssima performance e controle
* âš ï¸ Mais complexos, mas muito poderosos



## Como trocar?

Basta mudar a linha do modelo:

```python
# De:
modelo = LogisticRegression()

# Para, por exemplo:
modelo = RandomForestClassifier()
```

E manter o restante igual:

```python
modelo.fit(X_train, y_train)
y_pred = modelo.predict(X_test)
```

## ComparaÃ§Ãµes

VocÃª pode comparar vÃ¡rios modelos com:

* `accuracy_score`
* `classification_report`
* `confusion_matrix`
* `cross_val_score`

